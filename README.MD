# Comparative Emotion Detection System with NAO Robot â€“ CNN vs Facial/Postural Keypoints

Este proyecto implementa un sistema de detecciÃ³n emocional con el robot humanoide NAO, utilizando tres enfoques distintos de visiÃ³n por computadora: redes convolucionales (CNN), keypoints faciales y keypoints posturales.

El objetivo es comparar su precisiÃ³n, velocidad y robustez para identificar emociones bÃ¡sicas en niÃ±os con TEA, integrÃ¡ndolos en una interacciÃ³n real con el robot. El modelo con mejor desempeÃ±o se valida en el FabLab UPC en interacciÃ³n directa con niÃ±os autistas.

## ğŸš€ DescripciÃ³n General

El flujo del sistema es:

1. El NAO captura una imagen desde su cÃ¡mara frontal.
2. La imagen se codifica en base64 y se envÃ­a vÃ­a HTTP a una API Flask.
3. Un modelo de ML en la laptop analiza la emociÃ³n facial en la imagen.
4. La respuesta emocional se devuelve al NAO, que reacciona con voz y gestos expresivos.

## ğŸ¯ Objetivo

Comparar el rendimiento de tres tÃ©cnicas de reconocimiento emocional â€”CNN, keypoints faciales y keypoints posturalesâ€” bajo distintos escenarios de entrada y condiciones reales, evaluando su utilidad para asistir a interacciones psicoeducativas con niÃ±os autistas mediante el robot NAO.

## ğŸ§  TecnologÃ­as utilizadas

- **Robot**: NAO (versiÃ³n con Python 2.7)
- **API backend**: Flask (Python 3.10)
- **Modelos**:
  - CNN (MobileNetV2 / VGG19)
  - Keypoints faciales (MediaPipe + MLP)
  - Keypoints posturales (MediaPipe Pose + MLP)
- **LibrerÃ­as**:
  - `pynaoqi`
  - `Flask`, `NumPy`, `OpenCV`, `Pillow`
  - `MediaPipe`, `scikit-learn`, `PyTorch`
- **ComunicaciÃ³n**: HTTP simple (sin sockets ni ROS)

## ğŸ“ Estructura del repositorio

```
â”œâ”€â”€ models/ # Modelos para clasificaciÃ³n emocional
â”‚   â”œâ”€â”€ cnn_model.py
â”‚   â”œâ”€â”€ keypoints_face_model.py
â”‚   â””â”€â”€ keypoints_pose_model.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ datasets_used.md
â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ compare_metrics.ipynb
â”œâ”€â”€ main.py                 # Script que corre en NAO (Py2.7)
â”œâ”€â”€ run.sh                 # Script de ejecuciÃ³n     
â”œâ”€â”€ apiflask/              # Carpeta con backend Flask en Py3 (uso demodelo y API)
â”œâ”€â”€ pynaoqi-python2.7-2.8.6.23-linux64-20191127_152327/
â”œâ”€â”€ readme.md      
        
```

## âš™ï¸ Requisitos

TODO: add requirements 2.7 y 3.10


## â–¶ï¸ Uso

TODO: Detallar uso

1. Ejecutar la API Flask en la laptop:

```bash
cd apiflask
python app.py
```

2. Ejecutar el script en el NAO (configurar IP del NAO):

```bash
./run.sh
```

## ğŸ§ª Pruebas

Se probaron tres tÃ©cnicas de detecciÃ³n de emociones bajo diferentes condiciones controladas (iluminaciÃ³n, resoluciÃ³n, entrada recortada vs completa, color vs escala de grises).

Se midieron:

- PrecisiÃ³n y F1-score

- Velocidad de inferencia

- Robustez frente a condiciones no ideales

La tÃ©cnica con mejor desempeÃ±o se implementÃ³ en el NAO y se validÃ³ en una sesiÃ³n real con niÃ±os en el FabLab.
(Metricas, diseno de exprimento)

## ğŸ“š Enfoque acadÃ©mico

Se busca explorar cÃ³mo los sistemas de detecciÃ³n emocional pueden integrarse en contextos de robÃ³tica social como herramientas de soporte para niÃ±os con TEA, siguiendo enfoques vicarios.

Este proyecto se enmarca como investigaciÃ³n aplicada de carÃ¡cter comparativo, contribuyendo al anÃ¡lisis de tÃ©cnicas de visiÃ³n por computadora para contextos sociales asistidos. Se integra una validaciÃ³n prÃ¡ctica con niÃ±os autistas en el FabLab de la universidad, aportando evidencia preliminar sobre la viabilidad tÃ©cnica y pedagÃ³gica del enfoque.

## ğŸ“Š Resultados comparativos (TBD)

Se incluirÃ¡n tablas comparativas de rendimiento por tÃ©cnica y escenario. Las pruebas abarcan desde entornos controlados hasta validaciones ecolÃ³gicas con niÃ±os en el FabLab.
