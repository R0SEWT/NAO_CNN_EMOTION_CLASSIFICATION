üß† Descripci√≥n del Proyecto

NAO Emocional Interactivo

Objetivo:
Permitir que el robot NAO detecte emociones humanas (como felicidad, tristeza o confusi√≥n) y reaccione de forma adecuada, usando gestos corporales y habla.
üß© Componentes del Sistema
1. NAO (Python 2.7 con NAOqi)

    Captura una imagen con su c√°mara interna usando ALVideoDevice.

    Codifica la imagen en base64.

    Env√≠a la imagen v√≠a HTTP POST a un servidor Flask para clasificar la emoci√≥n.

    Recibe una emoci√≥n (happy, confused, no_face, etc.).

    Ejecuta un comportamiento preprogramado (hablar + mover brazos o cabeza) seg√∫n la emoci√≥n detectada.

2. Servidor Flask (Python 3.9)

    Recibe la imagen en base64 desde NAO.

    Decodifica y procesa la imagen.

    Usa un modelo preentrenado (modelo_emocion.h5) para detectar emociones.

    Devuelve la emoci√≥n clasificada al NAO.

    Guarda un log local con la imagen clasificada y su etiqueta, con timestamp.

üéØ Flujo General

    NAO dice una frase y captura una imagen.

    La env√≠a al servidor.

    El servidor responde con la emoci√≥n.

    NAO responde con un gesto y voz.

    La laptop guarda un log con la imagen.

üõ†Ô∏è Tecnolog√≠as
Componente	Tecnolog√≠a
Robot	NAOqi SDK (Python 2.7)
Backend	Flask (Python 3.9)
IA	Keras + TensorFlow (modelo .h5)
Comunicaci√≥n	HTTP POST (json con base64)
Logs	Archivos .jpg con nombre timestamp_emocion.jpg